{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import get_pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dictionary = get_pkl(f\"./data/pkls/train_queries.pkl\")\n",
    "train_queries = get_pkl(f\"./data/pkls/train_queries.pkl\")\n",
    "\n",
    "query_names, query_ids = [row[0] for row in train_queries], [row[1] for row in train_queries]\n",
    "dict_names, dict_ids = [row[0] for row in train_dictionary], [row[1] for row in train_dictionary]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\Desktop\\thesis\\1_2_biosyn_replicate\\git_repo_final\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from biosyn.biosyn import BioSyn\n",
    "\n",
    "\n",
    "biosyn = BioSyn(25, False, 20, 'dmis-lab/biobert-base-cased-v1.1' )\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = dict_names[:32]\n",
    "embs = biosyn.embed_dense_optimized(ns, return_tensor=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/pkls/dicts_1.json\") as f:\n",
    "    meta_d = json.load(f)\n",
    "with open(\"./data/pkls/queries_1.json\") as f:\n",
    "    meta_q = json.load(f)\n",
    "with open(\"./data/pkls/results_1.json\") as f:\n",
    "    meta_r = json.load(f)\n",
    "\n",
    "\n",
    "mm_d = np.memmap(\"./data/pkls/dicts_1.fp16.mmap\", \n",
    "                 dtype=np.float16 if meta_d[\"dtype\"] == \"fp16\" else np.float32, \n",
    "                 shape=(meta_d[\"N\"], meta_d[\"d\"]))\n",
    "\n",
    "\n",
    "mm_q = np.memmap(\"./data/pkls/queries_1.fp16.mmap\", \n",
    "                 dtype=np.float16 if meta_q[\"dtype\"] == \"fp16\" else np.float32, \n",
    "                 shape=((meta_r[\"N\"], meta_q[\"d\"])))\n",
    "\n",
    "mm_r = np.memmap(\"./data/pkls/results_1.mmap\", \n",
    "                 dtype=np.float16 if meta_r[\"dtype\"] == \"fp16\" else np.int32, \n",
    "                 shape=((meta_r[\"N\"], meta_r[\"d\"])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ids_sets = [set(s.split(\"|\")) if isinstance(s, str) else set(s) for s in dict_ids]\n",
    "query_id_tokens = [tuple(q.split(\"|\")) if isinstance(q, str) else tuple(q) for q in query_ids]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cui_to_dict_idx = {}\n",
    "for i, cui in enumerate(dict_ids):\n",
    "    toks = cui.split(\"|\") if isinstance(cui, str) else list(cui)\n",
    "    for t in toks: cui_to_dict_idx.setdefault(t, []).append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [1,3,6,7,25063,13,36013,36017]\n",
    "[dict_names[idx] for idx in ls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_idx_per_query = []\n",
    "labels_per_query = []\n",
    "for q_idx, cand_idxs in enumerate(mm_r):\n",
    "    q_id_tokens = query_id_tokens[q_idx]\n",
    "    possible = set(cui_to_dict_idx.get(q_id_tokens[0], []))\n",
    "    print(possible)\n",
    "    \n",
    "    if q_idx == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from biosyn.dataloader import CandidateDataset\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1' )\n",
    "ds = CandidateDataset(queries=train_queries, dicts=train_dictionary, tokenizer=tokenizer, max_length=25, topk=20, pre_tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_idx, query_cands in enumerate(mm_r):\n",
    "    print(f\"query_idx: {query_idx}\")\n",
    "    invalid = [id for id in query_cands if id > len(dict_ids)]\n",
    "    print(f\"invalid: {invalid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_arr = mm_r.flatten()\n",
    "# valid_indexes = flat_arr[flat_arr < len(dict_ids)]\n",
    "# len(valid_indexes)\n",
    "# len(flat_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "chunk = 12288\n",
    "\n",
    "N, d = meta_d[\"N\"], meta_d[\"d\"]\n",
    "index = faiss.IndexFlatIP(meta_d[\"d\"])\n",
    "\n",
    "\n",
    "#stream add to dict in chunks\n",
    "for s in range(0, N, chunk):\n",
    "    e = min(s+chunk, N)\n",
    "    part = mm_d[s:e]\n",
    "    t = part\n",
    "    index.add(t)\n",
    "    del t\n",
    "faiss_index = index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_pkl\n",
    "\n",
    "i_all = get_pkl(\"./data/pkls/I_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_d.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_flat = i_all.flatten()\n",
    "[idx for idx in i_flat if idx < len(dict_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "shape = (3,4)\n",
    "\n",
    "# np.empty(shape)\n",
    "\n",
    "arr_1 = np.empty(shape=shape )\n",
    "\n",
    "arr = np.memmap(shape=shape,mode=\"w+\", filename=\"draft_mmap.mmap\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[0, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.memmap(\"draft_mmap.mmap\", mode=\"r\", dtype=np.int32, shape=(3,4) )\n",
    "arr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

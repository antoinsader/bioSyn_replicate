{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90599/90599 [00:00<00:00, 1542375.78it/s]\n",
      "100%|██████████| 691/691 [00:00<00:00, 5135.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from biosyn.dataloader import load_dictionary, load_queries\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "TRAIN_DICT_PATH = \"./data/data-ncbi-fair/train_dictionary.txt\"\n",
    "TRAIN_DIR = \"./data/data-ncbi-fair/traindev\"\n",
    "\n",
    "train_dictionary  = load_dictionary(dict_path=TRAIN_DICT_PATH)\n",
    "train_queries  = load_queries(data_dir=TRAIN_DIR, filter_composite=False, filter_duplicates=False, filter_cuiless=True)\n",
    "\n",
    "train_dictionary = train_dictionary[:100]\n",
    "train_queries = train_queries[:10]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "encoder = AutoModel.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "\n",
    "\n",
    "max_length = 25\n",
    "\n",
    "\n",
    "query_names = [row[0] for row in train_queries]\n",
    "dict_names = [row[0] for row in train_dictionary]\n",
    "topk = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class CandidateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, queries, dicts, tokenizer, max_length, topk, pre_tokenize):\n",
    "        \"\"\"\n",
    "        Retrieve top-k candidates based on dense embedding\n",
    "        Parameters\n",
    "        ----------\n",
    "        queries : list\n",
    "            A list of tuples (name, id)\n",
    "        dicts : list\n",
    "            A list of tuples (name, id)\n",
    "        tokenizer : BertTokenizer\n",
    "            A BERT tokenizer for dense embedding\n",
    "        topk : int\n",
    "            The number of candidates\n",
    "        \"\"\"\n",
    "        self.query_names, self.query_ids = [row[0] for row in queries], [row[1] for row in queries]\n",
    "        self.dict_names, self.dict_ids = [row[0] for row in dicts], [row[1] for row in dicts]\n",
    "        self.tokenizer= tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.topk = topk\n",
    "        self.d_cand_idxs = None\n",
    "        self.pre_tokenize = pre_tokenize\n",
    "        if pre_tokenize:\n",
    "            all_query_names_tokens = self.tokenizer(self.query_names, max_length=max_length,padding='max_length', truncation=True, return_tensors='pt' )\n",
    "            self.all_query_names_tokens = [\n",
    "                {\n",
    "                    \"input_ids\": all_query_names_tokens[\"input_ids\"][idx],\n",
    "                    \"attention_mask\": all_query_names_tokens[\"attention_mask\"][idx],\n",
    "                } for  idx in range(len(all_query_names_tokens[\"input_ids\"]))]\n",
    "\n",
    "            self.all_dict_names_tokens= self.tokenizer(self.dict_names, max_length=max_length,padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "    def set_dense_candidate_idxs(self, d_cand_idxs):\n",
    "        self.d_cand_idxs = d_cand_idxs\n",
    "\n",
    "    def __getitem__(self, query_idx):\n",
    "        \"\"\"\n",
    "            Return (query_tokens, cand_tokens), labels\n",
    "            query_tokens: tokenized the query_name (query_name is query_names[query_idx] the specific mention)\n",
    "            cand_tokens: \n",
    "        \"\"\"\n",
    "        assert (self.d_cand_idxs is not None)\n",
    "\n",
    "        if self.pre_tokenize:\n",
    "            query_tokens = self.all_query_names_tokens[query_idx]\n",
    "        else:\n",
    "            query_name = self.query_names[query_idx]\n",
    "            query_tokens = self.tokenizer(query_name, max_length=self.max_length,padding='max_length', truncation=True, return_tensors='pt' )\n",
    "\n",
    "        d_cand_idxs = self.d_cand_idxs[query_idx]\n",
    "        topk_candidate_idx = np.array(d_cand_idxs)\n",
    "\n",
    "        assert len(topk_candidate_idx) == self.topk\n",
    "        assert len(topk_candidate_idx) == len(set(topk_candidate_idx))\n",
    "\n",
    "\n",
    "        if self.pre_tokenize:\n",
    "            cand_idxs_tensor = torch.as_tensor(topk_candidate_idx, dtype=torch.long)\n",
    "            cand_tokens = {\n",
    "                k: v.index_select(0, cand_idxs_tensor)\n",
    "                for k, v in self.all_dict_names_tokens.items()\n",
    "                if isinstance(v, torch.Tensor)\n",
    "            }\n",
    "        else:\n",
    "            cand_names = [self.dict_names[cand_idx] for cand_idx in topk_candidate_idx]\n",
    "            cand_tokens = self.tokenizer(cand_names, max_length=self.max_length, padding=\"max_length\" , truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        labels = self.get_labels(query_idx, topk_candidate_idx).astype(np.float32)\n",
    "\n",
    "        return (query_tokens, cand_tokens), labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.query_names)\n",
    "\n",
    "    def check_label(self, query_id, candidate_id_set):\n",
    "        \"\"\"\n",
    "            check if all q_id in query_id.split(\"|\") exists in candidate_id_set \n",
    "        \"\"\"\n",
    "        label = 0\n",
    "        query_ids = query_id.split(\"|\")\n",
    "        for q_id in query_ids:\n",
    "            if q_id in candidate_id_set:\n",
    "                label = 1\n",
    "                continue\n",
    "            else:\n",
    "                label = 0\n",
    "                break\n",
    "        return label\n",
    "    \n",
    "    def get_labels(self, query_idx, candidate_idxs):\n",
    "        labels = np.array([])\n",
    "        query_id = self.query_ids[query_idx]\n",
    "        candidate_ids = np.array(self.dict_ids)[candidate_idxs]\n",
    "        for candidate_id in candidate_ids:\n",
    "            label = self.check_label(query_id, candidate_id)\n",
    "            labels = np.append(labels, label)\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cand_idxs = np.array([\n",
    "#     np.random.choice(len(train_dictionary), size=topk, replace=False  )\n",
    "#     for _ in range(len(train_queries))\n",
    "# ], dtype=np.int64)\n",
    "\n",
    "cand_idxs = np.array([[ 6, 58, 40, 55],\n",
    "       [60, 23, 21, 56],\n",
    "       [25, 85,  3, 74],\n",
    "       [64, 32, 14, 86],\n",
    "       [14, 99, 90, 10],\n",
    "       [72, 37, 56, 83],\n",
    "       [14, 13, 44, 20],\n",
    "       [83, 11, 42, 41],\n",
    "       [11, 61, 54, 68],\n",
    "       [62, 32, 84, 90]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CandidateDataset(\n",
    "        queries = train_queries, \n",
    "        dicts = train_dictionary, \n",
    "        tokenizer = tokenizer, \n",
    "        max_length = max_length, \n",
    "        topk= topk,\n",
    "        pre_tokenize = True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "train_set.set_dense_candidate_idxs(cand_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=10, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: tensor([[144.6959, 144.7281, 153.0147, 152.2294],\n",
      "        [148.9545, 148.6415, 153.7524, 156.2342],\n",
      "        [142.7202, 148.5990, 134.1674, 144.6824],\n",
      "        [152.7010, 152.6938, 143.5929, 148.1381],\n",
      "        [141.6751, 152.5371, 150.6783, 141.5227],\n",
      "        [158.3967, 151.0699, 151.5319, 156.7922],\n",
      "        [143.5973, 144.2186, 149.0260, 142.3616],\n",
      "        [144.0762, 157.9371, 151.7619, 154.5725],\n",
      "        [145.2944, 139.2736, 142.4796, 151.8815],\n",
      "        [140.9750, 143.5386, 144.1806, 143.8584]], grad_fn=<SqueezeBackward1>) \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, data in enumerate(train_loader):\n",
    "    x, batch_y = data\n",
    "    query_token, candidate_tokens = x\n",
    "    batch_size, topk, max_length = candidate_tokens['input_ids'].shape\n",
    "    query_embed = encoder(\n",
    "            input_ids=query_token[\"input_ids\"].squeeze(1),\n",
    "            attention_mask=query_token[\"attention_mask\"].squeeze(1),\n",
    "        )\n",
    "    query_embed = query_embed[0][:,0].unsqueeze(1) #(B, 1, H) ??????? WHY [0] last hidden state\n",
    "\n",
    "\n",
    "    candidate_embeds = encoder(\n",
    "        input_ids=candidate_tokens['input_ids'].reshape(-1, max_length),\n",
    "        attention_mask=candidate_tokens['attention_mask'].reshape(-1, max_length)\n",
    "    )\n",
    "    candidate_embeds = candidate_embeds[0][:,0].reshape(batch_size, topk, -1) # [batch_size, topk, hidden]\n",
    "\n",
    "\n",
    "    score_1 = torch.bmm(query_embed, candidate_embeds.permute(0,2,1)).squeeze(1)\n",
    "    print(f\"score: {score_1} \" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: tensor([[144.6959, 144.7281, 153.0147, 152.2294],\n",
      "        [148.9545, 148.6415, 153.7524, 156.2342],\n",
      "        [142.7202, 148.5990, 134.1674, 144.6824],\n",
      "        [152.7010, 152.6938, 143.5929, 148.1381],\n",
      "        [141.6751, 152.5371, 150.6783, 141.5227],\n",
      "        [158.3967, 151.0699, 151.5319, 156.7922],\n",
      "        [143.5973, 144.2186, 149.0260, 142.3616],\n",
      "        [144.0762, 157.9371, 151.7619, 154.5725],\n",
      "        [145.2944, 139.2736, 142.4796, 151.8815],\n",
      "        [140.9750, 143.5386, 144.1806, 143.8584]], grad_fn=<SqueezeBackward1>) \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, data in enumerate(train_loader):\n",
    "    x, batch_y = data\n",
    "    query_token, candidate_tokens = x\n",
    "    batch_size, topk, max_length = candidate_tokens['input_ids'].shape\n",
    "    query_embed = encoder(**query_token, return_dict=True)\n",
    "    query_embed = query_embed.last_hidden_state[:, 0, :].unsqueeze(1) #(B, 1, H\n",
    "\n",
    "\n",
    "    candidate_embeds = encoder(\n",
    "        input_ids=candidate_tokens['input_ids'].reshape(-1, max_length),\n",
    "        attention_mask=candidate_tokens['attention_mask'].reshape(-1, max_length)\n",
    "    )\n",
    "    candidate_embeds = candidate_embeds[0][:,0].reshape(batch_size, topk, -1) # [batch_size, topk, hidden]\n",
    "\n",
    "\n",
    "    score_2 = torch.bmm(query_embed, candidate_embeds.permute(0,2,1)).squeeze(1)\n",
    "    print(f\"score: {score_2} \" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(score_1, score_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
